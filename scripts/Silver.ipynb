{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb012007-ab33-41c9-96fe-0d8d29fce69d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Atualizando os Pontos de Montagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f47e85e-40ed-4af3-8f0a-62d799379710",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=2546323010824681#setting/sparkui/0621-010009-au9o8kfu/driver-1530063440948448265\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=2546323010824681#setting/sparkui/0621-010009-au9o8kfu/driver-1530063440948448265\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.refreshMounts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e233d75-fd2d-442a-a085-bb59f3edf995",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Definindo uma função para montar um ADLS com um ponto de montagem com ADLS SAS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "058a3cc4-370c-4abf-8c60-39a7f3f302d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storageAccountName = \"datalakea554ff8ebf909b7f\"\n",
    "storageAccountAccessKey = \"\"\n",
    "sasToken = \"sv=2024-11-04&ss=bfqt&srt=sco&sp=rwdlacupyx&se=2025-06-21T08:52:06Z&st=2025-06-21T00:52:06Z&spr=https&sig=o8hXtkjAZe2AnVEy5dNqQl4LSl0Nkac98k6pJrV1NfI%3D\"\n",
    "\n",
    "def mount_adls(blobContainerName):\n",
    "    try:\n",
    "      dbutils.fs.mount(\n",
    "        source = \"wasbs://{}@{}.blob.core.windows.net\".format(blobContainerName, storageAccountName),\n",
    "        mount_point = f\"/mnt/{storageAccountName}/{blobContainerName}\",\n",
    "        #extra_configs = {'fs.azure.account.key.' + storageAccountName + '.blob.core.windows.net': storageAccountAccessKey}\n",
    "        extra_configs = {'fs.azure.sas.' + blobContainerName + '.' + storageAccountName + '.blob.core.windows.net': sasToken}\n",
    "      )\n",
    "      print(\"OK!\")\n",
    "    except Exception as e:\n",
    "      print(\"Falha\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fced822b-8bab-437b-8a58-add8a4acd24a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Mostrando os pontos de montagem no cluster Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d53984ff-8536-4a33-9e43-acfe34489ab1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>mountPoint</th><th>source</th><th>encryptionType</th></tr></thead><tbody><tr><td>/mnt/datalake5ee525b259fdd7be/silver</td><td>wasbs://silver@datalake5ee525b259fdd7be.blob.core.windows.net</td><td></td></tr><tr><td>/databricks-datasets</td><td>databricks-datasets</td><td></td></tr><tr><td>/mnt/datalake56cea6a3510650a6/landing-zone</td><td>wasbs://landing-zone@datalake56cea6a3510650a6.blob.core.windows.net</td><td></td></tr><tr><td>/mnt/datalakebae112c35115d7e3/bronze</td><td>wasbs://bronze@datalakebae112c35115d7e3.blob.core.windows.net</td><td></td></tr><tr><td>/mnt/datalake56cea6a3510650a6/gold</td><td>wasbs://gold@datalake56cea6a3510650a6.blob.core.windows.net</td><td></td></tr><tr><td>/mnt/datalakea554ff8ebf909b7f/lading-zone</td><td>wasbs://lading-zone@datalakea554ff8ebf909b7f.blob.core.windows.net</td><td></td></tr><tr><td>/mnt/datalake9fc5ac43334b8d99/landing-zone</td><td>wasbs://landing-zone@datalake9fc5ac43334b8d99.blob.core.windows.net</td><td></td></tr><tr><td>/mnt/datalakecd63092f4c645dfd/lading-zone</td><td>wasbs://lading-zone@datalakecd63092f4c645dfd.blob.core.windows.net</td><td></td></tr><tr><td>/mnt/datalakea554ff8ebf909b7f/silver</td><td>wasbs://silver@datalakea554ff8ebf909b7f.blob.core.windows.net</td><td></td></tr><tr><td>/mnt/datalake5ee525b259fdd7be/gold</td><td>wasbs://gold@datalake5ee525b259fdd7be.blob.core.windows.net</td><td></td></tr><tr><td>/mnt/datalake56cea6a3510650a6/lading-zone</td><td>wasbs://lading-zone@datalake56cea6a3510650a6.blob.core.windows.net</td><td></td></tr><tr><td>/databricks/mlflow-tracking</td><td>databricks/mlflow-tracking</td><td>sse-s3</td></tr><tr><td>/mnt/datalake5ee525b259fdd7be/landing-zone</td><td>wasbs://landing-zone@datalake5ee525b259fdd7be.blob.core.windows.net</td><td></td></tr><tr><td>/databricks-results</td><td>databricks-results</td><td>sse-s3</td></tr><tr><td>/mnt/datalakebae112c35115d7e3/landing-zone</td><td>wasbs://landing-zone@datalakebae112c35115d7e3.blob.core.windows.net</td><td></td></tr><tr><td>/mnt/datalakebae112c35115d7e3/lading-zone</td><td>wasbs://lading-zone@datalakebae112c35115d7e3.blob.core.windows.net</td><td></td></tr><tr><td>/mnt/datalakebae112c35115d7e3/silver</td><td>wasbs://silver@datalakebae112c35115d7e3.blob.core.windows.net</td><td></td></tr><tr><td>/mnt/datalake7bda6b3f393108d9/gold</td><td>wasbs://gold@datalake7bda6b3f393108d9.blob.core.windows.net</td><td></td></tr><tr><td>/mnt/datalakecd63092f4c645dfd/silver</td><td>wasbs://silver@datalakecd63092f4c645dfd.blob.core.windows.net</td><td></td></tr><tr><td>/databricks/mlflow-registry</td><td>databricks/mlflow-registry</td><td>sse-s3</td></tr><tr><td>/mnt/datalake9fc5ac43334b8d99/bronze</td><td>wasbs://bronze@datalake9fc5ac43334b8d99.blob.core.windows.net</td><td></td></tr><tr><td>/mnt/datalake4f4de806e56e47cc/lading-zone</td><td>wasbs://lading-zone@datalake4f4de806e56e47cc.blob.core.windows.net</td><td></td></tr><tr><td>/mnt/datalake4f4de806e56e47cc/landing-zone</td><td>wasbs://landing-zone@datalake4f4de806e56e47cc.blob.core.windows.net</td><td></td></tr><tr><td>/mnt/datalakecd63092f4c645dfd/landing-zone</td><td>wasbs://landing-zone@datalakecd63092f4c645dfd.blob.core.windows.net</td><td></td></tr><tr><td>/mnt/datalake7bda6b3f393108d9/silver</td><td>wasbs://silver@datalake7bda6b3f393108d9.blob.core.windows.net</td><td></td></tr><tr><td>/mnt/datalake4f4de806e56e47cc/silver</td><td>wasbs://silver@datalake4f4de806e56e47cc.blob.core.windows.net</td><td></td></tr><tr><td>/mnt/datalakea554ff8ebf909b7f/bronze</td><td>wasbs://bronze@datalakea554ff8ebf909b7f.blob.core.windows.net</td><td></td></tr><tr><td>/mnt/datalake4f4de806e56e47cc/gold</td><td>wasbs://gold@datalake4f4de806e56e47cc.blob.core.windows.net</td><td></td></tr><tr><td>/mnt/datalakecd63092f4c645dfd/gold</td><td>wasbs://gold@datalakecd63092f4c645dfd.blob.core.windows.net</td><td></td></tr><tr><td>/mnt/datalakea554ff8ebf909b7f/gold</td><td>wasbs://gold@datalakea554ff8ebf909b7f.blob.core.windows.net</td><td></td></tr><tr><td>/mnt/datalake56cea6a3510650a6/bronze</td><td>wasbs://bronze@datalake56cea6a3510650a6.blob.core.windows.net</td><td></td></tr><tr><td>/mnt/datalakecd63092f4c645dfd/bronze</td><td>wasbs://bronze@datalakecd63092f4c645dfd.blob.core.windows.net</td><td></td></tr><tr><td>/mnt/datalake5ee525b259fdd7be/bronze</td><td>wasbs://bronze@datalake5ee525b259fdd7be.blob.core.windows.net</td><td></td></tr><tr><td>/mnt/datalake56cea6a3510650a6/silver</td><td>wasbs://silver@datalake56cea6a3510650a6.blob.core.windows.net</td><td></td></tr><tr><td>/mnt/datalake7bda6b3f393108d9/landing-zone</td><td>wasbs://landing-zone@datalake7bda6b3f393108d9.blob.core.windows.net</td><td></td></tr><tr><td>/mnt/datalake7bda6b3f393108d9/bronze</td><td>wasbs://bronze@datalake7bda6b3f393108d9.blob.core.windows.net</td><td></td></tr><tr><td>/</td><td>DatabricksRoot</td><td>sse-s3</td></tr><tr><td>/mnt/datalakebae112c35115d7e3/gold</td><td>wasbs://gold@datalakebae112c35115d7e3.blob.core.windows.net</td><td></td></tr><tr><td>/mnt/datalake4f4de806e56e47cc/bronze</td><td>wasbs://bronze@datalake4f4de806e56e47cc.blob.core.windows.net</td><td></td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "/mnt/datalake5ee525b259fdd7be/silver",
         "wasbs://silver@datalake5ee525b259fdd7be.blob.core.windows.net",
         ""
        ],
        [
         "/databricks-datasets",
         "databricks-datasets",
         ""
        ],
        [
         "/mnt/datalake56cea6a3510650a6/landing-zone",
         "wasbs://landing-zone@datalake56cea6a3510650a6.blob.core.windows.net",
         ""
        ],
        [
         "/mnt/datalakebae112c35115d7e3/bronze",
         "wasbs://bronze@datalakebae112c35115d7e3.blob.core.windows.net",
         ""
        ],
        [
         "/mnt/datalake56cea6a3510650a6/gold",
         "wasbs://gold@datalake56cea6a3510650a6.blob.core.windows.net",
         ""
        ],
        [
         "/mnt/datalakea554ff8ebf909b7f/lading-zone",
         "wasbs://lading-zone@datalakea554ff8ebf909b7f.blob.core.windows.net",
         ""
        ],
        [
         "/mnt/datalake9fc5ac43334b8d99/landing-zone",
         "wasbs://landing-zone@datalake9fc5ac43334b8d99.blob.core.windows.net",
         ""
        ],
        [
         "/mnt/datalakecd63092f4c645dfd/lading-zone",
         "wasbs://lading-zone@datalakecd63092f4c645dfd.blob.core.windows.net",
         ""
        ],
        [
         "/mnt/datalakea554ff8ebf909b7f/silver",
         "wasbs://silver@datalakea554ff8ebf909b7f.blob.core.windows.net",
         ""
        ],
        [
         "/mnt/datalake5ee525b259fdd7be/gold",
         "wasbs://gold@datalake5ee525b259fdd7be.blob.core.windows.net",
         ""
        ],
        [
         "/mnt/datalake56cea6a3510650a6/lading-zone",
         "wasbs://lading-zone@datalake56cea6a3510650a6.blob.core.windows.net",
         ""
        ],
        [
         "/databricks/mlflow-tracking",
         "databricks/mlflow-tracking",
         "sse-s3"
        ],
        [
         "/mnt/datalake5ee525b259fdd7be/landing-zone",
         "wasbs://landing-zone@datalake5ee525b259fdd7be.blob.core.windows.net",
         ""
        ],
        [
         "/databricks-results",
         "databricks-results",
         "sse-s3"
        ],
        [
         "/mnt/datalakebae112c35115d7e3/landing-zone",
         "wasbs://landing-zone@datalakebae112c35115d7e3.blob.core.windows.net",
         ""
        ],
        [
         "/mnt/datalakebae112c35115d7e3/lading-zone",
         "wasbs://lading-zone@datalakebae112c35115d7e3.blob.core.windows.net",
         ""
        ],
        [
         "/mnt/datalakebae112c35115d7e3/silver",
         "wasbs://silver@datalakebae112c35115d7e3.blob.core.windows.net",
         ""
        ],
        [
         "/mnt/datalake7bda6b3f393108d9/gold",
         "wasbs://gold@datalake7bda6b3f393108d9.blob.core.windows.net",
         ""
        ],
        [
         "/mnt/datalakecd63092f4c645dfd/silver",
         "wasbs://silver@datalakecd63092f4c645dfd.blob.core.windows.net",
         ""
        ],
        [
         "/databricks/mlflow-registry",
         "databricks/mlflow-registry",
         "sse-s3"
        ],
        [
         "/mnt/datalake9fc5ac43334b8d99/bronze",
         "wasbs://bronze@datalake9fc5ac43334b8d99.blob.core.windows.net",
         ""
        ],
        [
         "/mnt/datalake4f4de806e56e47cc/lading-zone",
         "wasbs://lading-zone@datalake4f4de806e56e47cc.blob.core.windows.net",
         ""
        ],
        [
         "/mnt/datalake4f4de806e56e47cc/landing-zone",
         "wasbs://landing-zone@datalake4f4de806e56e47cc.blob.core.windows.net",
         ""
        ],
        [
         "/mnt/datalakecd63092f4c645dfd/landing-zone",
         "wasbs://landing-zone@datalakecd63092f4c645dfd.blob.core.windows.net",
         ""
        ],
        [
         "/mnt/datalake7bda6b3f393108d9/silver",
         "wasbs://silver@datalake7bda6b3f393108d9.blob.core.windows.net",
         ""
        ],
        [
         "/mnt/datalake4f4de806e56e47cc/silver",
         "wasbs://silver@datalake4f4de806e56e47cc.blob.core.windows.net",
         ""
        ],
        [
         "/mnt/datalakea554ff8ebf909b7f/bronze",
         "wasbs://bronze@datalakea554ff8ebf909b7f.blob.core.windows.net",
         ""
        ],
        [
         "/mnt/datalake4f4de806e56e47cc/gold",
         "wasbs://gold@datalake4f4de806e56e47cc.blob.core.windows.net",
         ""
        ],
        [
         "/mnt/datalakecd63092f4c645dfd/gold",
         "wasbs://gold@datalakecd63092f4c645dfd.blob.core.windows.net",
         ""
        ],
        [
         "/mnt/datalakea554ff8ebf909b7f/gold",
         "wasbs://gold@datalakea554ff8ebf909b7f.blob.core.windows.net",
         ""
        ],
        [
         "/mnt/datalake56cea6a3510650a6/bronze",
         "wasbs://bronze@datalake56cea6a3510650a6.blob.core.windows.net",
         ""
        ],
        [
         "/mnt/datalakecd63092f4c645dfd/bronze",
         "wasbs://bronze@datalakecd63092f4c645dfd.blob.core.windows.net",
         ""
        ],
        [
         "/mnt/datalake5ee525b259fdd7be/bronze",
         "wasbs://bronze@datalake5ee525b259fdd7be.blob.core.windows.net",
         ""
        ],
        [
         "/mnt/datalake56cea6a3510650a6/silver",
         "wasbs://silver@datalake56cea6a3510650a6.blob.core.windows.net",
         ""
        ],
        [
         "/mnt/datalake7bda6b3f393108d9/landing-zone",
         "wasbs://landing-zone@datalake7bda6b3f393108d9.blob.core.windows.net",
         ""
        ],
        [
         "/mnt/datalake7bda6b3f393108d9/bronze",
         "wasbs://bronze@datalake7bda6b3f393108d9.blob.core.windows.net",
         ""
        ],
        [
         "/",
         "DatabricksRoot",
         "sse-s3"
        ],
        [
         "/mnt/datalakebae112c35115d7e3/gold",
         "wasbs://gold@datalakebae112c35115d7e3.blob.core.windows.net",
         ""
        ],
        [
         "/mnt/datalake4f4de806e56e47cc/bronze",
         "wasbs://bronze@datalake4f4de806e56e47cc.blob.core.windows.net",
         ""
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "mountPoint",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "source",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "encryptionType",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dbutils.fs.mounts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f6ec630-db65-41cb-9fdf-a941cfafe400",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Mostrando todos os arquivos da camada bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "295f6535-5ab1-43af-a1d3-fadec36b766b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-199240776736766>:1\u001b[0m\n",
       "\u001b[0;32m----> 1\u001b[0m \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdbutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/mnt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mstorageAccountName\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/bronze\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
       "\n",
       "File \u001b[0;32m/databricks/python_shell/dbruntime/display.py:86\u001b[0m, in \u001b[0;36mDisplay.display\u001b[0;34m(self, input, *args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     83\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_custom_display_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf)\n",
       "\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mlist\u001b[39m):\n",
       "\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m)\n",
       "\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpandas.core.frame\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
       "\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession\u001b[38;5;241m.\u001b[39mcreateDataFrame(\u001b[38;5;28minput\u001b[39m))\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n",
       "\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n",
       "\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n",
       "\u001b[1;32m     51\u001b[0m     )\n",
       "\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/session.py:1216\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n",
       "\u001b[1;32m   1211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n",
       "\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n",
       "\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n",
       "\u001b[1;32m   1214\u001b[0m         data, schema, samplingRatio, verifySchema\n",
       "\u001b[1;32m   1215\u001b[0m     )\n",
       "\u001b[0;32m-> 1216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n",
       "\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n",
       "\u001b[1;32m   1218\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/session.py:1266\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n",
       "\u001b[1;32m   1264\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n",
       "\u001b[1;32m   1265\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[0;32m-> 1266\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m   1267\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
       "\u001b[1;32m   1268\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39mapplySchemaToPythonRDD(jrdd\u001b[38;5;241m.\u001b[39mrdd(), struct\u001b[38;5;241m.\u001b[39mjson())\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/session.py:888\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n",
       "\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_createFromLocal\u001b[39m(\n",
       "\u001b[1;32m    881\u001b[0m     \u001b[38;5;28mself\u001b[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001b[38;5;28mstr\u001b[39m]]]\n",
       "\u001b[1;32m    882\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[Tuple]\u001b[39m\u001b[38;5;124m\"\u001b[39m, StructType]:\n",
       "\u001b[1;32m    883\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
       "\u001b[1;32m    884\u001b[0m \u001b[38;5;124;03m    Create an RDD for DataFrame from a list or pandas.DataFrame, returns the RDD and schema.\u001b[39;00m\n",
       "\u001b[1;32m    885\u001b[0m \u001b[38;5;124;03m    This would be broken with table acl enabled as user process does not have permission to\u001b[39;00m\n",
       "\u001b[1;32m    886\u001b[0m \u001b[38;5;124;03m    write temp files.\u001b[39;00m\n",
       "\u001b[1;32m    887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
       "\u001b[0;32m--> 888\u001b[0m     internal_data, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrap_data_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m    889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39mparallelize(internal_data), struct\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/session.py:861\u001b[0m, in \u001b[0;36mSparkSession._wrap_data_schema\u001b[0;34m(self, data, schema)\u001b[0m\n",
       "\u001b[1;32m    858\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n",
       "\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
       "\u001b[0;32m--> 861\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchemaFromList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m    862\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n",
       "\u001b[1;32m    863\u001b[0m     tupled_data: Iterable[Tuple] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(converter, data)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/session.py:748\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n",
       "\u001b[1;32m    733\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
       "\u001b[1;32m    734\u001b[0m \u001b[38;5;124;03mInfer schema from list of Row, dict, or tuple.\u001b[39;00m\n",
       "\u001b[1;32m    735\u001b[0m \n",
       "\u001b[0;32m   (...)\u001b[0m\n",
       "\u001b[1;32m    745\u001b[0m \u001b[38;5;124;03m:class:`pyspark.sql.types.StructType`\u001b[39;00m\n",
       "\u001b[1;32m    746\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
       "\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n",
       "\u001b[0;32m--> 748\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan not infer schema from empty dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
       "\u001b[1;32m    749\u001b[0m infer_dict_as_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf\u001b[38;5;241m.\u001b[39minferDictAsStruct()\n",
       "\u001b[1;32m    750\u001b[0m prefer_timestamp_ntz \u001b[38;5;241m=\u001b[39m is_timestamp_ntz_preferred()\n",
       "\n",
       "\u001b[0;31mValueError\u001b[0m: can not infer schema from empty dataset"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\nFile \u001b[0;32m<command-199240776736766>:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdbutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/mnt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mstorageAccountName\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/bronze\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\nFile \u001b[0;32m/databricks/python_shell/dbruntime/display.py:86\u001b[0m, in \u001b[0;36mDisplay.display\u001b[0;34m(self, input, *args, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_custom_display_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpandas.core.frame\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession\u001b[38;5;241m.\u001b[39mcreateDataFrame(\u001b[38;5;28minput\u001b[39m))\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/sql/session.py:1216\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1215\u001b[0m     )\n\u001b[0;32m-> 1216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/sql/session.py:1266\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1264\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1266\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1267\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n\u001b[1;32m   1268\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39mapplySchemaToPythonRDD(jrdd\u001b[38;5;241m.\u001b[39mrdd(), struct\u001b[38;5;241m.\u001b[39mjson())\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/sql/session.py:888\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_createFromLocal\u001b[39m(\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28mself\u001b[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001b[38;5;28mstr\u001b[39m]]]\n\u001b[1;32m    882\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[Tuple]\u001b[39m\u001b[38;5;124m\"\u001b[39m, StructType]:\n\u001b[1;32m    883\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;124;03m    Create an RDD for DataFrame from a list or pandas.DataFrame, returns the RDD and schema.\u001b[39;00m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;124;03m    This would be broken with table acl enabled as user process does not have permission to\u001b[39;00m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;124;03m    write temp files.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 888\u001b[0m     internal_data, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrap_data_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39mparallelize(internal_data), struct\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/sql/session.py:861\u001b[0m, in \u001b[0;36mSparkSession._wrap_data_schema\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    858\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 861\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchemaFromList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[1;32m    863\u001b[0m     tupled_data: Iterable[Tuple] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(converter, data)\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/sql/session.py:748\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;124;03mInfer schema from list of Row, dict, or tuple.\u001b[39;00m\n\u001b[1;32m    735\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;124;03m:class:`pyspark.sql.types.StructType`\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m--> 748\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan not infer schema from empty dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m infer_dict_as_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf\u001b[38;5;241m.\u001b[39minferDictAsStruct()\n\u001b[1;32m    750\u001b[0m prefer_timestamp_ntz \u001b[38;5;241m=\u001b[39m is_timestamp_ntz_preferred()\n\n\u001b[0;31mValueError\u001b[0m: can not infer schema from empty dataset",
       "errorSummary": "<span class='ansi-red-fg'>ValueError</span>: can not infer schema from empty dataset",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dbutils.fs.ls(f\"/mnt/{storageAccountName}/bronze\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ef3c883-3b0f-44cb-8917-49f1f1fd3819",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Gerando um dataframe dos delta lake no container bronze do Azure Data Lake Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37774595-e2cd-4ba0-92aa-4a6f239c0e7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-199240776736768>:1\u001b[0m\n",
       "\u001b[0;32m----> 1\u001b[0m df_autor   \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstorageAccountName\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/bronze/autor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
       "\u001b[1;32m      2\u001b[0m df_cliente   \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstorageAccountName\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/bronze/cliente\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
       "\u001b[1;32m      3\u001b[0m df_editora   \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstorageAccountName\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/bronze/editora\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n",
       "\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n",
       "\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n",
       "\u001b[1;32m     51\u001b[0m     )\n",
       "\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:302\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n",
       "\u001b[1;32m    300\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
       "\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n",
       "\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n",
       "\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n",
       "\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
       "\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n",
       "\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
       "\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    230\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
       "\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n",
       "\u001b[1;32m    232\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n",
       "\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n",
       "\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
       "\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
       "\n",
       "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: /mnt/datalakea554ff8ebf909b7f/bronze/autor."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\nFile \u001b[0;32m<command-199240776736768>:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_autor   \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstorageAccountName\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/bronze/autor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m df_cliente   \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstorageAccountName\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/bronze/cliente\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m df_editora   \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstorageAccountName\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/bronze/editora\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:302\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\nFile \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\n\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: /mnt/datalakea554ff8ebf909b7f/bronze/autor.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [PATH_NOT_FOUND] Path does not exist: /mnt/datalakea554ff8ebf909b7f/bronze/autor.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_autor   = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/autor\")\n",
    "df_cliente   = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/cliente\")\n",
    "df_editora   = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/editora\")\n",
    "df_endereco   = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/endereco\")\n",
    "df_estoque   = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/estoque\")\n",
    "df_funcionario   = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/funcionario\")\n",
    "df_item_pedido   = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/item_pedido\")\n",
    "df_livro  = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/livro\")\n",
    "df_pagamento  = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/pagamento\")\n",
    "df_pedido  = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/pedido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4ffc23f-8add-4747-b572-b470a7e5eb8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Adicionando metadados de data e hora de processamento e nome do arquivo de origem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f278962e-c91a-4900-ba19-27d9b9e08947",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-199240776736770>:3\u001b[0m\n",
       "\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m current_timestamp, lit\n",
       "\u001b[0;32m----> 3\u001b[0m df_autor   \u001b[38;5;241m=\u001b[39m df_apolice\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_hora_silver\u001b[39m\u001b[38;5;124m\"\u001b[39m, current_timestamp())\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnome_arquivo\u001b[39m\u001b[38;5;124m\"\u001b[39m, lit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautor\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
       "\u001b[1;32m      4\u001b[0m df_cliente     \u001b[38;5;241m=\u001b[39m df_carro\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_hora_silver\u001b[39m\u001b[38;5;124m\"\u001b[39m, current_timestamp())\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnome_arquivo\u001b[39m\u001b[38;5;124m\"\u001b[39m, lit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcliente\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
       "\u001b[1;32m      5\u001b[0m df_editora   \u001b[38;5;241m=\u001b[39m df_cliente\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_hora_silver\u001b[39m\u001b[38;5;124m\"\u001b[39m, current_timestamp())\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnome_arquivo\u001b[39m\u001b[38;5;124m\"\u001b[39m, lit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meditora\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
       "\n",
       "\u001b[0;31mNameError\u001b[0m: name 'df_apolice' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\nFile \u001b[0;32m<command-199240776736770>:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m current_timestamp, lit\n\u001b[0;32m----> 3\u001b[0m df_autor   \u001b[38;5;241m=\u001b[39m df_apolice\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_hora_silver\u001b[39m\u001b[38;5;124m\"\u001b[39m, current_timestamp())\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnome_arquivo\u001b[39m\u001b[38;5;124m\"\u001b[39m, lit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautor\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      4\u001b[0m df_cliente     \u001b[38;5;241m=\u001b[39m df_carro\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_hora_silver\u001b[39m\u001b[38;5;124m\"\u001b[39m, current_timestamp())\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnome_arquivo\u001b[39m\u001b[38;5;124m\"\u001b[39m, lit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcliente\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      5\u001b[0m df_editora   \u001b[38;5;241m=\u001b[39m df_cliente\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_hora_silver\u001b[39m\u001b[38;5;124m\"\u001b[39m, current_timestamp())\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnome_arquivo\u001b[39m\u001b[38;5;124m\"\u001b[39m, lit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meditora\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\n\u001b[0;31mNameError\u001b[0m: name 'df_apolice' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'df_apolice' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "df_autor   = df_autor.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"autor\"))\n",
    "df_cliente     = df_cliente.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"cliente\"))\n",
    "df_editora   = df_editora.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"editora\"))\n",
    "df_endereco  = df_endereco.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"endereco\"))\n",
    "df_estoque  = df_estoque.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"estoque\"))\n",
    "df_funcionario  = df_funcionario.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"funcionario\"))\n",
    "df_item_pedido  = df_item_pedido.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"item_pedido\"))\n",
    "df_livro  = df_livro.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"livro\"))\n",
    "df_pagamento  = df_pagamento.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"pagamento\"))\n",
    "df_pedido  = df_pedido.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"pedido\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35ef7fda-3ad5-447a-892e-16429040fa37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Fucao para renomear as colunas de cada delta table para maiuscula e ajusta o nome das colunas de acordo com o dicionario de dados\n",
    "def renomear_colunas(diretorio):\n",
    "\n",
    "    # Carregue o DataFrame a partir do delta\n",
    "    df = spark.read.format('delta').load(diretorio)\n",
    "\n",
    "    tabela = diretorio.split('/')[-2]\n",
    "\n",
    "    # Renomeie todas as colunas para maiúsculas e faça a substituição do padrão \"cd_\" para \"codigo_\"\n",
    "    for coluna in df.columns:\n",
    "        novo_nome = coluna.upper()\n",
    "        novo_nome = novo_nome.replace(\"CD_\", \"CODIGO_\")\n",
    "        novo_nome = novo_nome.replace(\"VL_\", \"VALOR_\")\n",
    "        novo_nome = novo_nome.replace(\"DT_\", \"DATA_\")\n",
    "        novo_nome = novo_nome.replace(\"NM_\", \"NOME_\")\n",
    "        novo_nome = novo_nome.replace(\"DS_\", \"DESCRICAO_\")\n",
    "        novo_nome = novo_nome.replace(\"NR_\", \"NUMERO_\")\n",
    "        novo_nome = novo_nome.replace(\"_UF\", \"_UNIDADE_FEDERATIVA\")\n",
    "        df = df.withColumnRenamed(coluna, novo_nome)\n",
    "        df = df.drop(\"DATA_HORA_BRONZE\")\n",
    "        df = df.drop(\"NOME_ARQUIVO\")\n",
    "        df = df.withColumn(\"NOME_ARQUIVO_BRONZE\", lit(tabela))\n",
    "        df = df.withColumn(\"DATA_ARQUIVO_SILVER\", current_timestamp())\n",
    "\n",
    "    # Salve o DataFrame modificado de volta no mesmo local\n",
    "    #df.display()\n",
    "    df.write.format('delta').save(f\"/mnt/{storageAccountName}/silver/{tabela}\")\n",
    "\n",
    "#Funcao que chama a funcao renomear colunas para todos os arquivos contidos no container\n",
    "def renomear_arquivos_delta(diretorio):\n",
    "\n",
    "    # Lista para armazenar os nomes dos arquivos delta\n",
    "    nomes_arquivos_delta = []\n",
    "\n",
    "    # Lista os arquivos no diretório\n",
    "    arquivos = dbutils.fs.ls(diretorio)\n",
    "    \n",
    "    # Itera sobre os arquivos e armazena os nomes dos arquivos delta\n",
    "    for arquivo in arquivos:\n",
    "        nome_arquivo = arquivo.path\n",
    "        renomear_colunas(nome_arquivo)\n",
    "\n",
    "    return nomes_arquivos_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f1daab8-07de-459e-ac48-855cb49780a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[179]: []"
     ]
    }
   ],
   "source": [
    "# Executa a funcao para atualizar todos os dataframes\n",
    "diretorio = f'/mnt/{storageAccountName}/bronze'\n",
    "\n",
    "renomear_arquivos_delta(diretorio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c725b8e-a396-4a66-b98a-c380f9be8901",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>CODIGO_SINISTRO</th><th>PLACA</th><th>DATA_SINISTRO</th><th>LOCAL_SINISTRO</th><th>CONDUTOR</th><th>NOME_ARQUIVO_BRONZE</th><th>DATA_ARQUIVO_SILVER</th></tr></thead><tbody><tr><td>1</td><td>UBM5337   </td><td>2024-09-11 00:00:00.000</td><td>4317558</td><td>VICENTE</td><td>sinistro</td><td>2024-05-03T05:13:02.390+0000</td></tr><tr><td>2</td><td>CXK4218   </td><td>2023-06-13 00:00:00.000</td><td>3303807</td><td>MARINA</td><td>sinistro</td><td>2024-05-03T05:13:02.390+0000</td></tr><tr><td>3</td><td>WOZ7113   </td><td>2023-11-24 00:00:00.000</td><td>4302055</td><td>BRUNO</td><td>sinistro</td><td>2024-05-03T05:13:02.390+0000</td></tr><tr><td>4</td><td>DMU6272   </td><td>2026-08-17 00:00:00.000</td><td>3548054</td><td>CAROLINE</td><td>sinistro</td><td>2024-05-03T05:13:02.390+0000</td></tr><tr><td>5</td><td>QWJ2410   </td><td>2025-12-24 00:00:00.000</td><td>1502855</td><td>VITOR</td><td>sinistro</td><td>2024-05-03T05:13:02.390+0000</td></tr><tr><td>6</td><td>UTD6060   </td><td>2026-03-05 00:00:00.000</td><td>3170305</td><td>JOÃO</td><td>sinistro</td><td>2024-05-03T05:13:02.390+0000</td></tr><tr><td>7</td><td>SCS8313   </td><td>2025-09-07 00:00:00.000</td><td>3515400</td><td>BENJAMIN</td><td>sinistro</td><td>2024-05-03T05:13:02.390+0000</td></tr><tr><td>8</td><td>IYU0603   </td><td>2024-06-02 00:00:00.000</td><td>2516201</td><td>DAVI</td><td>sinistro</td><td>2024-05-03T05:13:02.390+0000</td></tr><tr><td>9</td><td>DLO7218   </td><td>2024-06-17 00:00:00.000</td><td>3532702</td><td>NICOLE</td><td>sinistro</td><td>2024-05-03T05:13:02.390+0000</td></tr><tr><td>10</td><td>GSP5419   </td><td>2025-05-18 00:00:00.000</td><td>3505500</td><td>BIANCA</td><td>sinistro</td><td>2024-05-03T05:13:02.390+0000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1",
         "UBM5337   ",
         "2024-09-11 00:00:00.000",
         "4317558",
         "VICENTE",
         "sinistro",
         "2024-05-03T05:13:02.390+0000"
        ],
        [
         "2",
         "CXK4218   ",
         "2023-06-13 00:00:00.000",
         "3303807",
         "MARINA",
         "sinistro",
         "2024-05-03T05:13:02.390+0000"
        ],
        [
         "3",
         "WOZ7113   ",
         "2023-11-24 00:00:00.000",
         "4302055",
         "BRUNO",
         "sinistro",
         "2024-05-03T05:13:02.390+0000"
        ],
        [
         "4",
         "DMU6272   ",
         "2026-08-17 00:00:00.000",
         "3548054",
         "CAROLINE",
         "sinistro",
         "2024-05-03T05:13:02.390+0000"
        ],
        [
         "5",
         "QWJ2410   ",
         "2025-12-24 00:00:00.000",
         "1502855",
         "VITOR",
         "sinistro",
         "2024-05-03T05:13:02.390+0000"
        ],
        [
         "6",
         "UTD6060   ",
         "2026-03-05 00:00:00.000",
         "3170305",
         "JOÃO",
         "sinistro",
         "2024-05-03T05:13:02.390+0000"
        ],
        [
         "7",
         "SCS8313   ",
         "2025-09-07 00:00:00.000",
         "3515400",
         "BENJAMIN",
         "sinistro",
         "2024-05-03T05:13:02.390+0000"
        ],
        [
         "8",
         "IYU0603   ",
         "2024-06-02 00:00:00.000",
         "2516201",
         "DAVI",
         "sinistro",
         "2024-05-03T05:13:02.390+0000"
        ],
        [
         "9",
         "DLO7218   ",
         "2024-06-17 00:00:00.000",
         "3532702",
         "NICOLE",
         "sinistro",
         "2024-05-03T05:13:02.390+0000"
        ],
        [
         "10",
         "GSP5419   ",
         "2025-05-18 00:00:00.000",
         "3505500",
         "BIANCA",
         "sinistro",
         "2024-05-03T05:13:02.390+0000"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "CODIGO_SINISTRO",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "PLACA",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DATA_SINISTRO",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "LOCAL_SINISTRO",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "CONDUTOR",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "NOME_ARQUIVO_BRONZE",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DATA_ARQUIVO_SILVER",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.read.format('delta').load(f'/mnt/{storageAccountName}/silver/autor').limit(10).display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3210653005600686,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
